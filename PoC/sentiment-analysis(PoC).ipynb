{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3205803,"sourceType":"datasetVersion","datasetId":1918992}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **1. Import Libaries**\n Here we load all the Python packages needed to tame our data and text—silencing warnings, wrangling tables with pandas & NumPy, visualizing with Plotly, cleaning language with spaCy & NLTK, building ML pipelines with scikit-learn, and even calling in our trusty DeepRage toolkit. Strap in!","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.io as pio\npio.renderers.default = \"kaggle\"\n\nimport re\nimport spacy\nfrom emoji import replace_emoji\nimport numpy as np\nimport nltk\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import FunctionTransformer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nnltk.download('vader_lexicon', quiet=True)\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n!pip install --quiet wordfreq\n%pip install --quiet git+https://github.com/iseedeep/deeprage.git@main\nfrom deeprage.core import val_pie","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:45:48.941936Z","iopub.execute_input":"2025-05-06T14:45:48.942803Z","iopub.status.idle":"2025-05-06T14:46:20.157305Z","shell.execute_reply.started":"2025-05-06T14:45:48.94277Z","shell.execute_reply":"2025-05-06T14:46:20.156519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **2. Data Loading & EDA**\nHere we’ll load our financial-sentiment dataset, peek at its structure, handle duplicates/nulls, \n  and visualize the sentiment distribution with a pie chart. Think of it as laying out the battle map before the charge!","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/financial-sentiment-analysis/data.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:20.158819Z","iopub.execute_input":"2025-05-06T14:46:20.15951Z","iopub.status.idle":"2025-05-06T14:46:20.223167Z","shell.execute_reply.started":"2025-05-06T14:46:20.159486Z","shell.execute_reply":"2025-05-06T14:46:20.2225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:20.223979Z","iopub.execute_input":"2025-05-06T14:46:20.22431Z","iopub.status.idle":"2025-05-06T14:46:20.247222Z","shell.execute_reply.started":"2025-05-06T14:46:20.224283Z","shell.execute_reply":"2025-05-06T14:46:20.24649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:20.248082Z","iopub.execute_input":"2025-05-06T14:46:20.248611Z","iopub.status.idle":"2025-05-06T14:46:20.260286Z","shell.execute_reply.started":"2025-05-06T14:46:20.248584Z","shell.execute_reply":"2025-05-06T14:46:20.259411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:20.262226Z","iopub.execute_input":"2025-05-06T14:46:20.262708Z","iopub.status.idle":"2025-05-06T14:46:20.278617Z","shell.execute_reply.started":"2025-05-06T14:46:20.262686Z","shell.execute_reply":"2025-05-06T14:46:20.277912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop_duplicates(inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:20.279427Z","iopub.execute_input":"2025-05-06T14:46:20.279716Z","iopub.status.idle":"2025-05-06T14:46:20.295498Z","shell.execute_reply.started":"2025-05-06T14:46:20.27969Z","shell.execute_reply":"2025-05-06T14:46:20.294769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_pie(df, 'Sentiment')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:20.296443Z","iopub.execute_input":"2025-05-06T14:46:20.296664Z","iopub.status.idle":"2025-05-06T14:46:20.537433Z","shell.execute_reply.started":"2025-05-06T14:46:20.296644Z","shell.execute_reply":"2025-05-06T14:46:20.536463Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **2. Text Processing**\n Time to clean and expand our text: strip HTML and emojis, convert crypto slang acronyms to full phrases, \n  lemmatize with spaCy, and compute Vader sentiment scores. This pipeline turns raw chatter into model-ready features!","metadata":{}},{"cell_type":"code","source":"chat_word = {\n    'FOMO':  'Fear Of Missing Out',\n    'FUD':   'Fear Uncertainty Doubt',\n    'DYOR':  'Do Your Own Research',\n    'BTFD':  'Buy The Fucking Dip',\n    'HODL':  'Hold On For Dear Life',\n    'ATH':   'All Time High',\n    'ATL':   'All Time Low',\n    'IPO':   'Initial Public Offering',\n    'ROI':   'Return On Investment',\n    'EPS':   'Earnings Per Share',\n    'P/E':   'Price To Earnings Ratio',\n    'YTD':   'Year To Date',\n    'YOY':   'Year Over Year',\n    'QoQ':   'Quarter Over Quarter',\n    'SL':    'Stop Loss',\n    'TP':    'Take Profit',\n    'PT':    'Price Target',\n    'MCAP':  'Market Capitalization',\n    'VOL':   'Trading Volume',\n    'ETF':   'Exchange Traded Fund',\n    'CFD':   'Contract For Difference',\n    'MOON':  'To The Moon',\n    'BEAR':  'Bearish Sentiment',\n    'BULL':  'Bullish Sentiment',\n}\n\n_acro_pat = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in chat_word) + r')\\b',\n                       flags=re.IGNORECASE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:20.538289Z","iopub.execute_input":"2025-05-06T14:46:20.538544Z","iopub.status.idle":"2025-05-06T14:46:20.54458Z","shell.execute_reply.started":"2025-05-06T14:46:20.538517Z","shell.execute_reply":"2025-05-06T14:46:20.543899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AcronymExpander(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None): return self\n    def transform(self, X, y=None):\n        return [\n            _acro_pat.sub(lambda m: chat_word[m.group(1).upper()], txt)\n            for txt in X\n        ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:20.545364Z","iopub.execute_input":"2025-05-06T14:46:20.545601Z","iopub.status.idle":"2025-05-06T14:46:20.5638Z","shell.execute_reply.started":"2025-05-06T14:46:20.545575Z","shell.execute_reply":"2025-05-06T14:46:20.562939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\"])\n\nclass SpacyCleaner(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        out = []\n        for doc in nlp.pipe(X, batch_size=50):\n            toks = [\n                tok.lemma_.lower()\n                for tok in doc\n                if (\n                    not tok.is_stop         # drop “the”, “and”, etc.\n                    and not tok.is_punct     # drop punctuation\n                    and not tok.like_url     # drop URLs\n                    and not tok.like_email   # drop emails\n                    and tok.is_alpha         # only letters\n                    and tok.lemma_ != \"-PRON-\"\n                )\n            ]\n            out.append(\" \".join(toks))\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:20.564653Z","iopub.execute_input":"2025-05-06T14:46:20.564904Z","iopub.status.idle":"2025-05-06T14:46:21.385661Z","shell.execute_reply.started":"2025-05-06T14:46:20.564884Z","shell.execute_reply":"2025-05-06T14:46:21.384942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def strip_html_emoji(texts):\n    cleaned = []\n    for t in texts:\n        no_html  = re.sub(r'<.*?>', ' ', t)\n        no_emoji = replace_emoji(no_html, replace='')\n        cleaned.append(no_emoji)\n    return cleaned","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:21.386827Z","iopub.execute_input":"2025-05-06T14:46:21.387097Z","iopub.status.idle":"2025-05-06T14:46:21.391321Z","shell.execute_reply.started":"2025-05-06T14:46:21.387077Z","shell.execute_reply":"2025-05-06T14:46:21.390552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sia = SentimentIntensityAnalyzer()\ndef vader_scores(texts):\n    return np.array([[sia.polarity_scores(t)['compound']] for t in texts])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:21.39217Z","iopub.execute_input":"2025-05-06T14:46:21.392426Z","iopub.status.idle":"2025-05-06T14:46:21.419393Z","shell.execute_reply.started":"2025-05-06T14:46:21.392407Z","shell.execute_reply":"2025-05-06T14:46:21.418538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = Pipeline([\n    (\"strip\", FunctionTransformer(strip_html_emoji, validate=False)),\n    (\"acro\",  AcronymExpander()),\n    (\"clean\", SpacyCleaner()),\n    (\"tfidf\", TfidfVectorizer(\n        tokenizer=str.split,\n        preprocessor=lambda x: x,\n        token_pattern=None,\n        min_df=1,    # keep any token appearing ≥1 doc\n        max_df=1.0   # keep tokens no matter how common\n    ))\n])\n\n# Fit the pipeline on all sentences and transform into sparse TF-IDF features\nX_feats = pipe.fit_transform(df['Sentence'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:21.420297Z","iopub.execute_input":"2025-05-06T14:46:21.420551Z","iopub.status.idle":"2025-05-06T14:46:31.922672Z","shell.execute_reply.started":"2025-05-06T14:46:21.420532Z","shell.execute_reply":"2025-05-06T14:46:31.921857Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4. Modeling & Evaluation**\nFinally, we’ll reduce features to 3D via PCA for a snazzy 3D scatter, then train and compare several classifiers \n  (RandomForest, NB variants, Logistic, SVC). We’ll conclude with detailed classification reports and confusion matrices \n  to crown the sentiment champion!","metadata":{}},{"cell_type":"code","source":"# Prepare labels and split into train/test sets with stratification\nX = X_feats\ny = df[\"Sentiment\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2, \n    stratify=y, \n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:31.925362Z","iopub.execute_input":"2025-05-06T14:46:31.925613Z","iopub.status.idle":"2025-05-06T14:46:31.938452Z","shell.execute_reply.started":"2025-05-06T14:46:31.925593Z","shell.execute_reply":"2025-05-06T14:46:31.937502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_array = X_feats.toarray() if hasattr(X_feats, \"toarray\") else X_feats\n\n# Use PCA to project features into 3D for interactive exploration\npca     = PCA(n_components=3, random_state=42)\nX_3d    = pca.fit_transform(X_array)\n\n# Build a DataFrame for a fancy 3D scatter of the first 200 samples\nidx = np.arange(100, 300)\ndf3 = pd.DataFrame(\n    X_3d[idx],\n    columns=[\"x\",\"y\",\"z\"]\n)\ndf3[\"label\"] = np.array(y)[idx]\n\n# Plot it with Plotly—because 3D PCA is just plain cool\nfig = px.scatter_3d(df3, x=\"x\", y=\"y\", z=\"z\", color=\"label\",\n                    title=\"3D PCA of Text Features\")\n\ntarget = \"positive\"\nmatches = df3.index[df3[\"label\"] == target]\nif len(matches):\n    i = matches[0]\n    ann = dict(\n        x=df3.at[i,\"x\"], y=df3.at[i,\"y\"], z=df3.at[i,\"z\"],\n        text=target, showarrow=True, arrowhead=1\n    )\n    fig.update_layout(scene=dict(annotations=[ann]))\n\nfig.show(renderer='iframe')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:31.939465Z","iopub.execute_input":"2025-05-06T14:46:31.939777Z","iopub.status.idle":"2025-05-06T14:46:35.733974Z","shell.execute_reply.started":"2025-05-06T14:46:31.939749Z","shell.execute_reply":"2025-05-06T14:46:35.733077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define our classifier lineup and specify which need dense input\nmodels = {\n    \"RandomForest\" : RandomForestClassifier(n_estimators=100, random_state=42),\n    \"GaussianNB\"   : GaussianNB(),\n    \"MultinomialNB\": MultinomialNB(),     \n    \"LogisticReg\"  : LogisticRegression(max_iter=1000),\n    \"LinearSVC\"    : LinearSVC(max_iter=10000),\n}\n\n_dense_needed = {\"GaussianNB\", \"MultinomialNB\"}\n\n# Train each model, print classification reports to compare performance \nfor name, clf in models.items():\n    # Prepare data\n    if name in _dense_needed:\n        X_tr = X_train.toarray()\n        X_te = X_test.toarray()\n    else:\n        X_tr, X_te = X_train, X_test\n\n    # Train & predict\n    clf.fit(X_tr, y_train)\n    preds = clf.predict(X_te)\n\n    # Report\n    print(f\"\\n=== {name} ===\")\n    print(classification_report(y_test, preds, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:46:35.734945Z","iopub.execute_input":"2025-05-06T14:46:35.735586Z","iopub.status.idle":"2025-05-06T14:46:58.620636Z","shell.execute_reply.started":"2025-05-06T14:46:35.735559Z","shell.execute_reply":"2025-05-06T14:46:58.619734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate and display confusion‐matrix heatmaps for each classifier\nfor name, clf in models.items():\n    if name in _dense_needed:\n        X_te = X_test.toarray()\n    else:\n        X_te = X_test\n\n    # get predictions\n    y_pred = clf.predict(X_te)\n\n    # build confusion matrix\n    labels = clf.classes_\n    cm = confusion_matrix(y_test, y_pred, labels=labels)\n    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n\n    # plot\n    fig = ff.create_annotated_heatmap(\n        z=cm_df.values,\n        x=list(labels),\n        y=list(labels),\n        colorscale=\"Blues\",\n        showscale=True,\n        reversescale=True\n    )\n    fig.update_layout(\n        title=f\"{name} Confusion Matrix\",\n        xaxis_title=\"Predicted\",\n        yaxis_title=\"Actual\",\n        width=600, height=500\n    )\n    fig.show(renderer=\"iframe_connected\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:49:13.022357Z","iopub.execute_input":"2025-05-06T14:49:13.023301Z","iopub.status.idle":"2025-05-06T14:49:13.576284Z","shell.execute_reply.started":"2025-05-06T14:49:13.023267Z","shell.execute_reply":"2025-05-06T14:49:13.575326Z"}},"outputs":[],"execution_count":null}]}